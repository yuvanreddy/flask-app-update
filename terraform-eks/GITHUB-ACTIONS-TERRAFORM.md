# GitHub Actions - Terraform Infrastructure Management

**Workflow File:** `.github/workflows/terraform-infrastructure.yml`  
**Purpose:** Automated Terraform apply and destroy operations  
**Date:** October 9, 2025

---

## ğŸš€ Features

### **Terraform Operations:**
- âœ… **Plan** - Preview infrastructure changes
- âœ… **Apply** - Create/update infrastructure
- âœ… **Destroy** - Delete all infrastructure
- âœ… Auto-approve option for automation

### **Information Display:**
- âœ… EKS cluster details (name, version, endpoint)
- âœ… Worker node count
- âœ… VPC ID and networking info
- âœ… Access commands (kubeconfig update)
- âœ… Cost estimates
- âœ… Next steps guidance

### **Verification:**
- âœ… Post-deployment health checks
- âœ… Addon verification
- âœ… Load Balancer Controller check
- âœ… Node status validation

---

## ğŸ“‹ Prerequisites

### **1. AWS Credentials**
Ensure `AWS_ROLE_TO_ASSUME` secret is configured:
```
Settings â†’ Secrets and variables â†’ Actions â†’ New repository secret
Name: AWS_ROLE_TO_ASSUME
Value: arn:aws:iam::816069153839:role/github-deployer-role
```

### **2. Terraform State Backend**
S3 bucket for Terraform state:
```
Bucket: terraform-state-816069153839
Key: eks/terraform.tfstate
Region: us-east-1
```

### **3. Required Files**
- `terraform-eks/*.tf` files
- `terraform-eks/terraform.tfvars`

---

## ğŸ¯ How to Use

### **Method 1: Plan (Preview Changes)**

1. **Go to GitHub Actions:**
   ```
   Your Repository â†’ Actions â†’ Terraform Infrastructure Management
   ```

2. **Click "Run workflow"**

3. **Select action:** `plan`

4. **Click "Run workflow"**

5. **Review the plan** in workflow output

### **Method 2: Apply (Create Infrastructure)**

1. **Go to GitHub Actions**

2. **Click "Run workflow"**

3. **Select action:** `apply`

4. **Auto-approve:**
   - âœ… Check for automatic approval
   - â¬œ Uncheck to require manual confirmation

5. **Click "Run workflow"**

6. **Wait 15-20 minutes** for completion

7. **Check Summary tab** for cluster URLs and access info

### **Method 3: Destroy (Delete Infrastructure)**

1. **Go to GitHub Actions**

2. **Click "Run workflow"**

3. **Select action:** `destroy`

4. **Auto-approve:**
   - âœ… Check for automatic approval (âš ï¸ dangerous!)
   - â¬œ Uncheck for safety

5. **Click "Run workflow"**

6. **Wait 15-20 minutes** for completion

---

## ğŸ“Š Workflow Output

### **After Successful Apply:**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        INFRASTRUCTURE DEPLOYMENT SUCCESSFUL! ğŸ‰                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŒ EKS CLUSTER INFORMATION:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cluster Name:    flask-eks
â”‚ Region:          us-east-1
â”‚ Status:          ACTIVE
â”‚ Version:         1.28
â”‚ Endpoint:        https://1E25F8C93ACAA2E6DE45E360E8888F47.gr7.us-east-1.eks.amazonaws.com
â”‚ Worker Nodes:    2
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ” ACCESS CONFIGURATION:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Update kubeconfig:
â”‚   aws eks update-kubeconfig --name flask-eks --region us-east-1
â”‚
â”‚ Verify access:
â”‚   kubectl cluster-info
â”‚   kubectl get nodes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“¦ DEPLOYED RESOURCES:
â”œâ”€ VPC ID: vpc-xxxxxxxxxxxxxxxxx
â”œâ”€ EKS Cluster: flask-eks
â”œâ”€ OIDC Provider: oidc.eks.us-east-1.amazonaws.com/id/xxxxx
â”œâ”€ Worker Nodes: 2 (t3.medium)
â”œâ”€ Load Balancer Controller: Installed
â”œâ”€ EBS CSI Driver: Installed
â””â”€ VPC CNI: Installed

ğŸ¯ NEXT STEPS:
1. Update kubeconfig (command above)
2. Deploy application:
   kubectl apply -f k8s/
3. Deploy monitoring:
   Run 'Deploy Monitoring Stack' workflow
4. Get application URL:
   kubectl get ingress -n default

ğŸ’° ESTIMATED MONTHLY COST: ~$255
â”œâ”€ EKS Cluster: ~$73/month
â”œâ”€ EC2 Instances: ~$60/month
â”œâ”€ NAT Gateways: ~$100/month
â”œâ”€ Load Balancers: ~$20/month
â””â”€ Other: ~$2/month
```

### **After Successful Destroy:**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        INFRASTRUCTURE DESTROYED SUCCESSFULLY! âœ…               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ—‘ï¸  DELETED RESOURCES:
â”œâ”€ EKS Cluster: flask-eks
â”œâ”€ Worker Nodes: All terminated
â”œâ”€ VPC and Subnets: Deleted
â”œâ”€ NAT Gateways: Deleted
â”œâ”€ Load Balancers: Deleted
â”œâ”€ Security Groups: Deleted
â”œâ”€ IAM Roles: Deleted
â””â”€ All associated resources: Deleted

ğŸ’° COST SAVINGS: ~$255/month

âš ï¸  NOTE:
- All data has been permanently deleted
- Resources cannot be recovered
- Verify in AWS Console that all resources are gone

ğŸ”„ TO RECREATE:
Run this workflow again with action: 'apply'
```

---

## ğŸ“ˆ GitHub Actions Summary

The workflow creates a detailed summary:

### **Apply Summary:**

| Property | Value |
|----------|-------|
| **Cluster Name** | flask-eks |
| **Region** | us-east-1 |
| **Status** | ACTIVE |
| **Version** | 1.28 |
| **Worker Nodes** | 2 |
| **VPC ID** | vpc-xxxxx |

**Access Commands:**
```bash
aws eks update-kubeconfig --name flask-eks --region us-east-1
kubectl cluster-info
kubectl get nodes
```

**Deployed Components:**
- âœ… VPC with public/private subnets
- âœ… EKS Cluster
- âœ… Worker Nodes (2x t3.medium)
- âœ… AWS Load Balancer Controller
- âœ… EBS CSI Driver
- âœ… VPC CNI Plugin
- âœ… OIDC Provider

---

## ğŸ”§ Configuration

### **Environment Variables:**

Edit in workflow file:
```yaml
env:
  AWS_REGION: us-east-1
  TF_VERSION: '1.6.0'
  WORKING_DIR: terraform-eks
```

### **Terraform Variables:**

Edit `terraform-eks/terraform.tfvars`:
```hcl
cluster_name    = "flask-eks"
cluster_version = "1.28"
region          = "us-east-1"
vpc_cidr        = "10.0.0.0/16"

node_groups = {
  general = {
    instance_types = ["t3.medium"]
    min_size       = 2
    max_size       = 4
    desired_size   = 2
  }
}
```

---

## ğŸ“š Workflow Jobs

### **Job 1: Terraform Operations**

**Steps:**
1. Checkout code
2. Configure AWS credentials (OIDC)
3. Setup Terraform
4. Format check
5. Initialize Terraform
6. Validate configuration
7. **Plan / Apply / Destroy** (based on input)
8. Get infrastructure outputs
9. Update kubeconfig
10. Get cluster information
11. Display infrastructure info
12. Create deployment summary

### **Job 2: Post-Apply Verification**

**Steps:**
1. Verify EKS cluster status
2. Check worker nodes
3. Verify addons (VPC CNI, CoreDNS, kube-proxy, EBS CSI)
4. Verify Load Balancer Controller
5. Create verification summary

---

## ğŸ¯ Use Cases

### **Use Case 1: Initial Infrastructure Setup**

```yaml
Action: apply
Auto-approve: âœ… (for automation)

Result:
âœ… EKS cluster created
âœ… Worker nodes launched
âœ… Networking configured
âœ… Addons installed
âœ… Ready for application deployment
```

### **Use Case 2: Infrastructure Update**

```bash
# 1. Edit terraform files
# 2. Commit and push
git add terraform-eks/
git commit -m "Update node count to 3"
git push origin main

# 3. Run workflow
Action: plan (review changes)
# Then
Action: apply (apply changes)
```

### **Use Case 3: Cleanup**

```yaml
Action: destroy
Auto-approve: âœ… (if sure) or â¬œ (for safety)

Result:
âœ… All resources deleted
âœ… Costs stopped
âœ… Clean slate
```

---

## ğŸ› Troubleshooting

### **Issue 1: Apply Fails with "Cluster Unreachable"**

**Cause:** Helm provider trying to connect to non-existent cluster

**Solution:**
```bash
# Remove Helm release from state
terraform state rm helm_release.aws_load_balancer_controller

# Re-run apply
```

### **Issue 2: Destroy Hangs**

**Possible Causes:**
- Load Balancers still exist
- Security Groups in use
- ENIs not deleted

**Solution:**
```bash
# Manually delete Load Balancers
aws elbv2 describe-load-balancers --region us-east-1
aws elbv2 delete-load-balancer --load-balancer-arn <ARN>

# Wait 2-3 minutes, then re-run destroy
```

### **Issue 3: Nodes Not Ready**

**Check:**
```bash
kubectl get nodes
kubectl describe node <node-name>
```

**Common fixes:**
- Wait 5-10 minutes for nodes to initialize
- Check VPC CNI addon status
- Verify security groups allow node communication

### **Issue 4: OIDC Authentication Fails**

**Check:**
```bash
# Verify OIDC provider exists
aws iam list-open-id-connect-providers

# Check trust policy
aws iam get-role --role-name github-deployer-role
```

---

## ğŸ” Security Best Practices

### **1. Use OIDC (No Long-lived Credentials)**

Already configured:
```yaml
permissions:
  id-token: write
  contents: read

- uses: aws-actions/configure-aws-credentials@v4
  with:
    role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
```

### **2. Protect Main Branch**

```
Settings â†’ Branches â†’ Add rule
Branch name pattern: main
âœ“ Require pull request reviews
âœ“ Require status checks to pass
```

### **3. Require Manual Approval for Destroy**

**Don't auto-approve destroy:**
```yaml
# âŒ Dangerous
auto_approve: true

# âœ… Safe
auto_approve: false
```

### **4. Use Terraform State Locking**

Already configured in backend:
```hcl
terraform {
  backend "s3" {
    bucket         = "terraform-state-816069153839"
    key            = "eks/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "terraform-state-lock"
  }
}
```

---

## ğŸ“Š Cost Management

### **Estimated Costs:**

| Resource | Monthly Cost |
|----------|--------------|
| EKS Cluster | $73 |
| EC2 Instances (2x t3.medium) | $60 |
| NAT Gateways (3x) | $100 |
| Application Load Balancers | $20 |
| EBS Volumes | $2 |
| **Total** | **~$255** |

### **Cost Optimization:**

**Reduce NAT Gateway costs:**
```hcl
# Use single NAT Gateway
enable_nat_gateway     = true
single_nat_gateway     = true  # Instead of one per AZ
one_nat_gateway_per_az = false
```

**Use Spot Instances:**
```hcl
node_groups = {
  spot = {
    instance_types = ["t3.medium"]
    capacity_type  = "SPOT"
    min_size       = 2
    max_size       = 4
  }
}
```

---

## ğŸ¯ Complete Deployment Flow

### **Step 1: Deploy Infrastructure**
```
Run workflow: action = apply
Wait: 15-20 minutes
Result: EKS cluster ready
```

### **Step 2: Update Kubeconfig**
```bash
aws eks update-kubeconfig --name flask-eks --region us-east-1
kubectl get nodes
```

### **Step 3: Deploy Application**
```
Run workflow: build-deploy-app.yml
Wait: 5 minutes
Result: Application deployed
```

### **Step 4: Deploy Monitoring**
```
Run workflow: deploy-monitoring.yml
Wait: 10 minutes
Result: Grafana accessible
```

### **Step 5: Get URLs**
```bash
# Application URL
kubectl get ingress -n default

# Grafana URL
kubectl get svc -n monitoring prometheus-stack-grafana
```

---

## âœ… Checklist

**Before Apply:**
- [ ] AWS credentials configured
- [ ] Terraform state backend exists
- [ ] terraform.tfvars is correct
- [ ] Reviewed plan output

**After Apply:**
- [ ] Cluster status is ACTIVE
- [ ] Worker nodes are Ready
- [ ] Addons are installed
- [ ] kubeconfig updated
- [ ] Can access cluster with kubectl

**Before Destroy:**
- [ ] Backed up any important data
- [ ] Deleted application resources
- [ ] Deleted monitoring stack
- [ ] Confirmed with team
- [ ] Ready for costs to stop

---

## ğŸš€ Quick Start

**Deploy infrastructure in 3 steps:**

1. **Go to Actions:**
   ```
   https://github.com/yuvanreddy/flask-app-update/actions
   ```

2. **Select "Terraform Infrastructure Management"**

3. **Run workflow:**
   - Action: `apply`
   - Auto-approve: âœ…
   - Click "Run workflow"

4. **Wait 15-20 minutes**

5. **Check Summary for cluster info!** â­

6. **Update kubeconfig and deploy apps!** ğŸš€

---

**Created:** October 9, 2025  
**Workflow Version:** 1.0  
**Status:** Ready to use
